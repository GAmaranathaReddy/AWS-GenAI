{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AWS GenAI Learning","text":"<p>The goal of this learning is to give you hands-on experience in leveraging foundation models (FMs) through Amazon Bedrock. Amazon Bedrock is a fully managed service that provides access to FMs from third-party providers and Amazon; available via an API. With Bedrock, you can choose from a variety of models to find the one that\u2019s best suited for your use case.</p> <p>Within this series of labs, you will go through some of the most common Generative AI usage patterns we are seeing with our customers. You will explore techniques for generating text and images, and learn how to improve productivity by using foundation models to help in composing emails, summarizing text, answering questions, building chatbots, creating images, and generating code. You will gain hands-on experience using Bedrock APIs, SDKs, and open-source software, such as LangChain and FAISS, to implement these usage patterns.</p> <p>This learning is intended for developers and solution builders. For example:</p> <ul> <li>Financial, Data and Business Analysts</li> <li>Generative AI developers</li> <li>C-suite executives: Understand how guardrails enhance data security and compliance.</li> <li>Marketing and sales teams: Learn how guardrails maintain brand voice and customer engagement.</li> <li>IT and DevOps teams: Explore the technical implementation of guardrails.</li> <li>Legal teams: Delve into how guardrails can help in compliance with laws and regulations.</li> <li>Data scientists, Machine Learning engineers, Developers, Product managers: Dive into the technical details and implementation of guardrails.</li> <li>Security &amp; compliance teams: Understand how guardrails enhance data protection and compliance.</li> </ul> <p>What\u2019s included in this learning:</p> <ul> <li>Prompt Engineering</li> <li>Text Generation</li> <li>KnowledgeBase and RAG</li> <li>Model Customization</li> <li>Images and Multimodal</li> <li>Agents</li> <li>Open Source</li> </ul> <p></p> <p>You can choose the modules that apply to your use case. The modules are independent and have no dependency on each other.</p>"},{"location":"agents/","title":"Agents","text":""},{"location":"agents/#overview","title":"Overview","text":"<p>In this lab, you will learn about Agents for Amazon Bedrock, a new Amazon Bedrock capability that lets you harness the Foundation Model's (FM's) reasoning skills to execute multi-steps business tasks using natural language. You can simply state your problem, like \u201chelp me update my product catalogue\u201d and the agent breaks down the problem using the FM\u2019s reasoning capabilities and executes the steps to fulfill your request. You set up an agent with access to your organization\u2019s enterprise systems, processes, knowledge bases, and some building block functions. Then the agent comes up with the logic, figures out what APIs to call and when to call them, and completes the transactions in the right sequence. When an agent needs a piece of information from the user, it automatically asks the user for those additional details using natural language. And the best part about agents \u2014 it\u2019s leveraging the most up to date information you have and gives you relevant answers securely and privately.</p> <p>An agent consists of the following components:</p> <p>Foundation model \u2013 You choose a foundation model that the agent invokes to interpret user input and subsequent prompts in its orchestration process, and to generate responses and follow-up steps in its process.</p> <p>Instructions \u2013 You write up instructions that describe what the agent is designed to do. With advanced prompts, you can further customize instructions for the agent at every step of orchestration and include Lambda functions to parse the output of each step.</p> <p>(Optional) Action groups \u2013 You define the actions that the agent should carry out through providing two resources.</p> <ul> <li>An OpenAPI schema to define the APIs that the agent can invoke to carry out its tasks.</li> <li> <p>A Lambda function with the following input and output.         Input \u2013 The API and parameters identified during orchestration.         Output \u2013 The result of the API invocation.</p> </li> <li> <p>(Optional) Knowledge bases \u2013 Associate knowledge bases with an agent to allow it to query the knowledge base for extra context to augment response generation and input into steps of the orchestration process.</p> </li> </ul> <p>The following image schematizes the components of your agent.</p>"},{"location":"agents/#agents-architecture-pattern","title":"Agents Architecture pattern","text":""},{"location":"agents/#agents-components","title":"Agents components","text":""},{"location":"agents/#agents-patterns-with-knowledgebase","title":"Agent's Patterns with KnowledgeBase","text":"<p>In build-time, all these components are gathered to construct base prompts for the agent in order to carry out orchestration until the user request is completed. With advanced prompts, you can modify these base prompts with additional logic and few-shot examples to improve accuracy for each step of agent invocation. The base prompt templates contain instructions, action descriptions, knowledge base descriptions, and conversation history, all of which you can customize to modify the agent to the best of your needs. You then prepare your agent, which packages all the components of the agents, including security configurations, and brings the agent into a state where it is ready for testing in runtime.</p>"},{"location":"agents/#relevance","title":"Relevance","text":"<p>Generative AI applications often need to execute multistep tasks across company systems and data sources. Agents for Bedrock allows you to automatically orchestrate and analyze the task and break it down into the correct logical sequence using the FM\u2019s reasoning abilities. Agents automatically call the necessary APIs to transact with the company systems and processes to fulfill the request, determining along the way if they can proceed or if they need to gather more information. Using encryption in transit and at rest and IAM roles, agents provide secure access to enterprise data and APIs. With the Action Groups integration with AWS Lambda functions, agents lets you choose the implementation language for your API\u2019s connection. Thanks to the fully managed infrastructure provided, you don\u2019t have to worry about provisioning or managing infrastructure for your agent.</p>"},{"location":"agents/#target-audience","title":"Target Audience","text":"<p>Software Developers, Data Scientists, Solutions Architects and anyone else building Generative AI applications with access to internal APIs or Knowledge Bases</p>"},{"location":"agents/#challenges","title":"Challenges","text":"<p>Because agents architecture may include external components, debugging errors could be complex. This workshop will guide you through the debugging process of the most common challenges.</p>"},{"location":"agents/#sub-patterns","title":"Sub-patterns","text":"<p>During this workshop, you will cover three modules:</p> <ul> <li> <p>Building Agents for Insurance using API : Building Agents for Bedrock using Boto3 SDK: In this module, you will create agents for Bedrock programmatically using the insurance claim agent example. The files for this module are available in the insurance_claims_agent/without_kb folder.</p> </li> <li> <p>Integrating a Bedrock Knowledge Base to a Bedrock Agent : Integrating Knowledge Bases to your Agents: In this module, you will create and integrate a Knowledge Base to your insurance claims agent both via AWS console and via Boto3 SDK. The files for this module are available in the insurance_claims_agent/with_kb folder.</p> </li> <li> <p>Creating an agent using OpenSource via LangChain : Creating an Agent using LangChain: In this module, you will learn how to retrieve data automatically from APIs using an OpenSource solution based on LangChain.</p> </li> </ul>"},{"location":"aws-bedrock/","title":"Amazon Bedrock Workshop","text":"<p>The goal of this workshop is to give you hands-on experience in leveraging foundation models (FMs) through Amazon Bedrock. Amazon Bedrock is a fully managed service that provides access to FMs from third-party providers and Amazon; available via an API. With Bedrock, you can choose from a variety of models to find the one that\u2019s best suited for your use case.</p> <p>Within this series of labs, you will go through some of the most common Generative AI usage patterns we are seeing with our customers. You will explore techniques for generating text and images, and learn how to improve productivity by using foundation models to help in composing emails, summarizing text, answering questions, building chatbots, creating images, and generating code. You will gain hands-on experience using Bedrock APIs, SDKs, and open-source software, such as LangChain and FAISS, to implement these usage patterns.</p> <p>This workshop is intended for developers and solution builders. For example:</p> <ul> <li>Financial, Data and Business Analysts</li> <li>Generative AI developers</li> <li>C-suite executives: Understand how guardrails enhance data security and compliance.</li> <li>Marketing and sales teams: Learn how guardrails maintain brand voice and customer engagement.</li> <li>IT and DevOps teams: Explore the technical implementation of guardrails.</li> <li>Legal teams: Delve into how guardrails can help in compliance with laws and regulations.</li> <li>Data scientists, Machine Learning engineers, Developers, Product managers: Dive into the technical details and implementation of guardrails.</li> <li>Security &amp; compliance teams: Understand how guardrails enhance data protection and compliance.</li> </ul> <p>What\u2019s included in this workshop:</p> <ul> <li>Prompt Engineering</li> <li>Text Generation</li> <li>KnowledgeBase and RAG</li> <li>Model Customization</li> <li>Images and Multimodal</li> <li>Agents</li> <li>Open Source</li> </ul> <p></p> <p>You can choose the modules that apply to your use case. The modules are independent and have no dependency on each other.</p>"},{"location":"fine-tuning/","title":"Fine-tuning models","text":""},{"location":"fine-tuning/#customizing-your-models","title":"Customizing your models","text":""},{"location":"fine-tuning/#how-fms-benefit-from-fine-tuning","title":"How FMs benefit from fine-tuning","text":""},{"location":"fine-tuning/#customize-vs-augment","title":"Customize vs. augment","text":""},{"location":"fine-tuning/#fine-tuning-vs-retrieval-augmented-generation-rag","title":"Fine-tuning vs. Retrieval Augmented Generation (RAG)","text":""},{"location":"fine-tuning/#fine-tuning","title":"Fine-tuning","text":"<ul> <li>Is specific style, behavior, or vocabulary required?</li> <li>Is training data available?</li> <li>Do you need to reduce the risk of hallucinations?</li> </ul>"},{"location":"fine-tuning/#rag","title":"RAG","text":"<ul> <li>Is knowledge from external data sources required?</li> <li>Is data dynamic or changing?</li> <li>Do you need to know the sources of answers? </li> </ul>"},{"location":"fine-tuning/#custom-models-in-amazon-bedrock","title":"Custom models in Amazon Bedrock","text":""},{"location":"fine-tuning/#components-of-a-customization-job","title":"Components of a customization job","text":""},{"location":"fine-tuning/#customization-architecture-overview","title":"Customization architecture overview","text":""},{"location":"fine-tuning/#security-and-privacy","title":"Security and privacy","text":"<ul> <li>You are always in control of your data</li> <li>Data not used to improve models, and not shared with model providers</li> <li>Customer data remain in Region</li> <li>Support for AWS PrivateLink and VPC configurations</li> <li>Integration with AWS IAM</li> <li>API monitoring in AWS CloudTrail, logging &amp; metrics in Amazon CloudWatch</li> <li>Custom models encrypted and stored with Service or Customer Managed Keys (CMK) - Only you have access to your models</li> </ul>"},{"location":"fine-tuning/#customizing-model-responses-for-your-business","title":"Customizing model responses for your business","text":""},{"location":"fine-tuning/#datasets-for-instruction","title":"Datasets for instruction","text":""},{"location":"fine-tuning/#model-fine-tuning","title":"Model fine-tuning","text":"<ul> <li>Place in S3 your dataset for train, and validation</li> <li>jsonl format comprised of instructions { \u201dprompt\": \u201dinput text\u201d, \u201dcompletion\": \u201doutput text\u201d } Up to 10k training records, and 1k validation records</li> <li>Set hyperparameters</li> <li>Epoch: 1-10</li> <li>Batch size: defaults to 1</li> <li>Learning rate: defaults to 0.00005</li> <li>Learning warmup steps: recommended 0</li> </ul>"},{"location":"fine-tuning/#model-continued-pre-training","title":"Model continued pre-training","text":"<ul> <li>Place in S3 your dataset for train, and validation</li> <li>jsonl format comprised of unlabeled data { \u201dinput\": \u201dinput text\u201d} Up to 100k training records, and 1k validation records</li> <li>Set hyperparameters</li> <li>Epoch: 1-10</li> <li>Batch size: defaults to 1</li> <li>Learning rate: defaults to 0.00005</li> <li>Learning warmup steps: recommended 0</li> </ul>"},{"location":"image-multimodal/","title":"Image and MultiModal applications","text":""},{"location":"image-multimodal/#overview","title":"Overview","text":"<p>Image generation can be a tedious task for artists, designers and content creators who illustrate their thoughts with the help of images. With the help of Foundation Models (FMs) this tedious task can be streamlined to just a single line of text that expresses the thoughts of the artist, FMs can be used for creating realistic and artistic images of various subjects, environments, and scenes from language prompts.</p> <p>Image indexing and searching is another tedious enterprise task. With the help of FMs, enterprise can build multimodal image indexing, searching and recommendation applications quickly.</p> <p>In this lab, we will explore how to use FMs available in Amazon Bedrock to generate images as well as modify existing images, and how to use FMs to do multimodal image indexing and searching.</p>"},{"location":"image-multimodal/#relevance","title":"Relevance","text":"<p>Amazon Titan Image Generator G1 is an image generation model. It generates images from text, and allows users to upload and edit an existing image. Users can edit an image with a text prompt (without a mask) or parts of an image with an image mask. You can extend the boundaries of an image with outpainting, and fill in an image with inpainting. It can also generate variations of an image based on an optional text prompt. Amazon Titan Image Generator G1 Generator includes watermarking on the output files.</p> <p>Text-to-image (T2I) generation \u2013 Input a text prompt and generate a new image as output. The generated image captures the concepts described by the text prompt.</p> <p>Inpainting \u2013 Uses an image and a segmentation mask as input (either from the user or estimated by the model) and reconstructs the region within the mask. Use inpainting to remove masked elements and replace them with background pixels.</p> <p>Outpainting \u2013 Uses an image and a segmentation mask as input (either from the user or estimated by the model) and generates new pixels that seamlessly extend the region. Use precise outpainting to preserve the pixels of the masked image when extending the image to the boundaries. Use default outpainting to extend the pixels of the masked image to the image boundaries based on segmentation settings.</p> <p>Image variation \u2013 Uses an image and an optional prompt as input. It generates a new image that preserves the content of the input</p>"},{"location":"image-multimodal/#audience","title":"Audience","text":"<ul> <li>Marketing teams which need to generate images</li> <li>Multiple business units needing data from various internal documents which have a mix of tables and images and charts.</li> <li>Research units which need access to multi-form data</li> <li>Senior leadership who need access to crystalized summary from multiple forms of data.</li> </ul>"},{"location":"image-multimodal/#challenges","title":"Challenges","text":"<p>Data is coming in multiple forms which include images, charts, tables and text. All LLM's are great at textual data and hence the struggle is to create data from the disparate sources which can be used by LLM's to generate meaningful responses to the query. Maintaining relevant answers as conversations grow in complexity and scope.</p>"},{"location":"image-multimodal/#sub-patterns","title":"Sub-patterns","text":"<p>This lab explores the following patterns:</p> <ul> <li> <p>Convert text and prompts into Images  - Bedrock provides access to variety of image generation models like StableDiffusion and Titan Image generation.</p> </li> <li> <p>Titan Image Generator-Text to Image  - Bedrock Titan image generator model provides ability to generate high quality images from the input text. This model adds an invisible watermark to all generated images to reduce the spread of misinformation, assist with copyright protection, and track content usage</p> </li> <li> <p>Titan Multimodal Images  - Amazon Titan Multimodal Embedding Models can be used for enterprise tasks such as image search and similarity based recommendation from documents which include dis-parate information items like text, images, graphs.</p> </li> </ul>"},{"location":"knowledge-bases-rag/","title":"Knowledge Bases and RAG","text":""},{"location":"knowledge-bases-rag/#overview","title":"Overview","text":"<p>One of the key issues with LLM's is their ability to hallucinate, so returning answers grounded in facts becomes a top priority for question answering (QA), Contextual Chatbot and other use-cases. QA is an important task that involves extracting answers to factual queries posed in natural language. Typically, a QA system processes a query against a knowledge base containing structured or unstructured data and generates a response with accurate information. Ensuring high accuracy is key to developing a useful, reliable and trustworthy question answering system, especially for enterprise use cases.</p> <p>Generative AI models like Titan, Claude, and Jurassic use probability distributions to generate responses to questions. These models are trained on vast amounts of text data, which allows them to predict what comes next in a sequence or what word might follow a particular word. However, these models are not able to provide accurate or deterministic answers to every question because there is always some degree of uncertainty in the data.</p> <p>In this module, you will walk you through how to implement the QA pattern with Bedrock. Additionally, you will prepare embeddings to be loaded in the vector database (in this example, in notebook memory).</p>"},{"location":"knowledge-bases-rag/#relevance","title":"Relevance","text":"<p>Organisations need to query domain specific and proprietary data and use the information to answer questions, including data on which the model has not been trained. The challenge here is the limit on the amount of contextual information used in a given scenario, because models have a limit on the size of the prompt they can handle.</p> <p>You can overcome this challange using the Retrieval Augmented Generation (RAG) technique. RAG combines using embeddings to index the corpus of documents to build a knowledge base and using a large language model to extract information from a subset of documents in the knowledge base.</p> <p>As a preparation step for RAG, the documents comprising the knowledge base are split into chunks of a fixed size and passed to an embedding model to obtain the embedding vector. The size of each chunk is limited by the maximum input size of the chosen embedding model. The embedding together with the original chunk of the document and additional metadata are stored in a vector database. The vector database is optimized to efficiently perform similarity searches between vectors.</p>"},{"location":"knowledge-bases-rag/#target-audience","title":"Target Audience","text":"<p>Customers with data stores that may be private or frequently changing Customer service agents Anyone who needs to improve question search with accurate or specific documentation</p>"},{"location":"knowledge-bases-rag/#challenges","title":"Challenges","text":"<p>The challenges are getting relevant answers from a trained model and avoiding hallucination, as well as dealing with large amounts of data and model context limitations.</p>"},{"location":"knowledge-bases-rag/#introduction-to-knowledge-bases-for-amazon-bedrock","title":"Introduction to Knowledge Bases for Amazon Bedrock","text":"<p>Knowledge Bases for Amazon Bedrock With Knowledge Bases for Amazon Bedrock, you can give FMs and agents contextual information from your company\u2019s private data sources for Retrieval Augmented Generation (RAG) to deliver more relevant, accurate, and customized responses</p> <p>To equip FMs with up-to-date and proprietary information, organizations use Retrieval Augmented Generation (RAG), a technique that fetches data from company data sources and enriches the prompt to provide more relevant and accurate responses. Knowledge Bases for Amazon Bedrock is a fully managed capability that helps you implement the entire RAG workflow from ingestion to retrieval and prompt augmentation without having to build custom integrations to data sources and manage data flows. Session context management is built in, so your app can readily support multi-turn conversations.</p> <p>This example implements a knowledge base by specifying a data source such as Amazon S3, select an embedding model, such as Amazon Titan Embeddings to convert the data into vector embeddings, and a destination vector database such as Amazon OpenSearch Service Serverless (AOSS) to store the vector data. Bedrock takes care of creating, storing, managing, and updating your embeddings in the vector database.</p> <p></p> <p>This example implements the RAG architectural pattern. RAG retrieves data from outside the language model (non-parametric) and augments prompts by adding relevant retrieved data as context. Here, you are performing RAG effectively on the knowledge base created previously or using console.</p> <p>In first architectural pattern, you will use the RetreiveAndGenerate API provided by Knowledge Bases for Amazon Bedrock which converts user queries into embeddings, searches the knowledge base, get the relevant results, augment the prompt and then invoking a LLM to generate the response.</p> <p></p> <p>In second pattern architecture, you will use the Retreive API provided by Knowledge Bases for Amazon Bedrock which converts user queries into embeddings, searches the knowledge base, get the relevant results, giving you more control to build custom work\ufb02ows on top of the semantic search results. The output of the Retrieve API includes the the retrieved text chunks, the location type and URI of the source data, as well as the relevance scores of the retrievals. You will then use the text chunks being generated and augment it with the original prompt and pass it through the anthropic.claude-v2 model using prompt engineering patterns based on your use case.</p> <p></p> <p>You will also demonstrate the use of open source tools like Lang Chain to implement the RAG pattern. RAG retrieves data from outside the language model (non-parametric) and augments prompts by adding relevant retrieved data as context. Here, you are performing RAG effectively on the knowledge base created previously or using console/sdk. In this architecture, you will use the Retreive API provided by Knowledge Bases for Amazon Bedrock which converts user queries into embeddings, searches the knowledge base, get the relevant results, giving you more control to build custom work\ufb02ows on top of the semantic search results. The output of the Retrieve API includes the the retrieved text chunks, the location type and URI of the source data, as well as the relevance scores of the retrievals. You will then use the RetrievalQAChain provided by LangChain, add RetreiverAPI as a retriever in the chain. This chain will then automatically augment the text chunks being generated with the original prompt and pass it through the anthropic.claude-instant-v1 model.</p> <p></p> <p>Sub-patterns In this lab, you will explore multiple RAG patterns:</p> <ul> <li> <p>Ingest and Retrieve Pattern   - The first example will show how to create and ingest data using OpenSearch and then leverage RetrieveAndGenerate API of KnowledgeBase to search the embeddings and return contextual results. Behind the scenes, RetrieveAndGenerate API converts queries into embeddings, searches the knowledge base, and then augments the foundation model prompt with the search results as context information and returns the FM-generated response to the question. For multi-turn conversations, Knowledge Bases manage short-term memory of the conversation to provide more contextual results.</p> </li> <li> <p>Citations and Retrieve API provided by Knowledge Bases    - The Second pattern shows how to use Retrieve API provided by Knowledge Bases for Amazon Bedrock. Note that citation is extremely important to validate faithfulness and relevancy of the vector databases. The LLM can use the citation in conjunction with the right prompt template to generate response that is grounded in the facts provided by KnowledgeBase.</p> </li> <li> <p>Question Answering (Q&amp;A) with Retrieve API provided by Knowledge Bases    - The Third pattern shows how to build question answering applications using Retrieve API provided by Knowledge Bases for Amazon Bedrock. The LLM can leverage this API to gather relevant context, which, combined with an appropriate prompt template, allows generatation of responses grounded in the facts sourced from the KnowledgeBase.</p> </li> <li> <p>Question Answering with Retrieve API provided by Knowledge Bases with Claude V2  - This pattern will show the same pattern as above but with Claude V2, which needs a different prompt template.</p> </li> </ul>"},{"location":"model-customization/","title":"Model Customization","text":""},{"location":"model-customization/#overview","title":"Overview","text":"<p>Model customization is the process of providing training data to a model in order to improve its performance for specific use-cases. You can customize Amazon Bedrock foundation models in order to improve their performance and create a better customer experience. Amazon Bedrock currently provides the following customization methods.</p>"},{"location":"model-customization/#continued-pre-training","title":"Continued Pre-training","text":"<pre><code>Provide unlabeled data to pre-train a foundation model by familiarizing it with certain types of inputs. You can provide data from specific topics in order to expose a model to those areas. The Continued Pre-training process will tweak the model parameters to accommodate the input data and improve its domain knowledge.\n\nFor example, you can train a model with private data, such as business documents, that are not publically available for training large language models. Additionally, you can continue to improve the model by retraining the model with more unlabeled data as it becomes available.\n</code></pre>"},{"location":"model-customization/#fine-tuning","title":"Fine-tuning","text":"<pre><code>Provide labeled data in order to train a model to improve performance on specific tasks. By providing a training dataset of labeled examples, the model learns to associate what types of outputs should be generated for certain types of inputs. The model parameters are adjusted in the process and the model's performance is improved for the tasks represented by the training dataset.\n</code></pre>"},{"location":"model-customization/#relevance","title":"Relevance","text":"<p>Enterprises need to LLM to use domain specific and proprietary data and use the information to answer the user query</p> <p>The challenge here is that there is a limitation on the amount of contextual information that can be used in a given scenario based on the limited size of the prompt that most models can handle.</p> <p>This can be overcome by fine-tuning the model on your proprietary data. Bedrock provide a low code / no code approach to fine tune which abstracts all the complexity of the fine-tuning.</p>"},{"location":"model-customization/#target-audience","title":"Target Audience","text":"<ul> <li>Machine Learning Engineers wanting to rapidly experiment and test.</li> <li>Data Scientists wanting to fine tune models.</li> <li>Anyone who needs to improve question search with accurate or specific documentation</li> </ul>"},{"location":"model-customization/#challenges","title":"Challenges","text":"<p>The challenges are getting relevant answers from a trained model and avoiding hallucinations and working on use cases which cannot be solved by RAG</p>"},{"location":"model-customization/#introduction-to-fine-tuning-for-amazon-bedrock","title":"Introduction to Fine-tuning for Amazon Bedrock","text":"<p>To view details of fine-tuning models in Bedrock visit Fine Tuning models . There are 2 key pre-requsites for creating the data needed for fine-tuning a model. Before you can begin a model customization job, you need to minimally prepare a training dataset. Whether a validation dataset is supported and the format of your training and validation dataset depend on the following factors. <pre><code>The type of customization job (fine-tuning or Continued Pre-training).\nThe input and output modalities of the data.\n</code></pre></p>"},{"location":"model-customization/#sub-patterns","title":"Sub-patterns","text":"<p>In this lab, you will explore sub-patterns explained below:</p> <ul> <li> <p>Setup Data for fine-tuning  - The first example will show how to set up for running customization notebooks both for fine-tuning and continued pre-training using Amazon Bedrock.</p> </li> <li> <p>Fine-tune Titan Lite  - The second pattern will show to fine tune Titan Lite model.Amazon Titan Text Express and Amazon Titan Text Lite are large language models (LLMs) that help customers improve productivity and efficiency for an extensive range of text-related tasks, and offer price and performance options that are optimized for your needs. You can now access these Amazon Titan Text foundation models in Amazon Bedrock, which helps you easily build and scale generative AI applications with new text generation capabilities. Amazon Titan Text Express has a context length of up to 8,000 tokens, making it well-suited for a wide range of advanced tasks.</p> </li> <li> <p>Fine-tune Llama  - The third pattern will show to fine tune [Llama-13B model[(https://aws.amazon.com/bedrock/llama-2/ ) which has been pre-trained on over 2 trillion tokens. Finally we will evaluate the fine-tuned model performance using fmeval on the summarization accuracy metrics including METEOR, ROUGE for fine-tuning.</p> </li> <li> <p>Continued Pre-Train Titan models  - The fourth pattern will show to how to run a continued pre-train the titan model. Organizations want to build domain-specific applications that reflect the terminology of their business. However, many FMs are trained on large amounts of publicly available data and are not suited to highly specialized domains. To adapt FMs with knowledge more relevant to a domain, you can engage continued pre-training which leverages vast sets of unlabeled data. Continued pre-training in Bedrock helps address out-of-domain learning challenges by exposing models to new, diverse data, beyond their original training. With continued pre-training, you can expand the model\u2019s understanding to include the language used in your domain and improve the model\u2019s overall competency for your business.</p> </li> </ul>"},{"location":"open-source/","title":"Open Source With Bedrock","text":""},{"location":"open-source/#overview","title":"Overview","text":"<p>This module focuses on bringing the open source modules into Bedrock . Open source community has been at the fore-front of innovation like RAG, Agents to solve complex business problems. We will showcase major technologies like Langchain  and LlamaIndex . We will showcase RAG and Agents using these technologies.</p> <p>Entity extraction is an NLP technique that allows us to automatically extract specific data from naturally written text, such as news, emails, books, etc. That data can then later be saved to a database, used for lookup, or any other type of processing. There are many different techniques and approaches for entity extraction, but in this module we will focus on using LLMs for entity extraction. LLM entity extraction works by sending a prompt instruction to the model, asking it to extract entities you specify:</p>"},{"location":"open-source/#why-it-is-relevant","title":"Why it is Relevant","text":"<p>Open Source Within this series of labs, you will be taken through some of the most common usage patterns we are seeing with our customers for Generative AI. We will explore techniques for generating text and images, contextual chats, RAG, creating value for organizations by improving productivity. This is achieved by leveraging foundation models to help in composing emails, summarizing text, answering questions, building chatbots, creating images and generating code. You will gain hands-on experience using Bedrock APIs, SDKs, and open-source software for example LangChain and FAISS to implement these usage patterns.</p>"},{"location":"open-source/#target-audience","title":"Target Audience","text":"<p>This module can be executed by any developer familiar with Python, as well as data scientists and other technical people.</p> <ul> <li>Financial, Data and Business Analysts</li> <li>Gen Ai developers</li> <li>C-suite executives: Understand how guardrails enhance data security and compliance.</li> <li>Marketing and sales teams: Learn how guardrails maintain brand voice and customer engagement.</li> <li>IT and DevOps teams: Explore the technical implementation of guardrails.</li> <li>Legal teams: Delve into how guardrails can help in compliance with laws and regulations.</li> <li>Data scientists, Machine Learning engineers, Developers, Product managers: Dive into the technical details and implementation of guardrails.</li> <li>Security &amp; Compliance teams: Understand how guardrails enhance data protection and compliance.</li> </ul>"},{"location":"open-source/#challenges","title":"Challenges","text":"<p>Implementing guardrails for chatbots presents several challenges:</p> <ul> <li>Ensuring guardrails do not hinder chatbot performance or user experience.</li> <li>Keeping guardrails updated as the chatbot evolves.</li> <li>Balancing guardrails with flexibility and adaptability in chatbot responses.</li> <li>Consistent application of guardrails across all chatbot instances and platforms.</li> </ul>"},{"location":"open-source/#sub-patterns","title":"Sub-patterns","text":"<ul> <li>Node JS with Bedrock  : In this notebook you will use JavaScript to generate the image using Titan Image generator.</li> <li>Introduction to Agents : Understanding Inner workings of Agents: In this module we will learn how the agents concept works in general. The files for this are under weather_service_agent Agents Intro</li> <li>Agents with Tools  : In these set of examples we will use Claude v3 models with LangChain to showcase function calling and providing tools to Agents to execute the end to end user workflow.</li> <li>lang Graph Supervisor Agents  : In these set of examples we will use Claude v3 models with Lang-graph to demonstrate inter agents communication and leverage agents for complex orchesteration. We will demonstrate multi-agent query and examples</li> <li>Langchain with Chatbot  : In these set of examples we will use Titan and Claude and Llama models to show contextual chat and open source vector databases to answer query grounded in facts and reduce hallucinations.</li> <li>Chatbot with Pinecone : In these of examples we will showcase Knowledgebase with Claude, Titan with open source Postgres and Pinecone and FAISS databases.</li> <li>Bedrock &amp; NeMo Guardrails  : In this notebook, you will utilize Amazon Bedrock and NVIDIA's NeMo guardrails.</li> <li>Chat with your Long documents with LangChain  : In this notebook, you will leverage the key patterns with Amazon Bedrock Claude V3 with ability to process long documents while reducing latency issues.</li> </ul>"},{"location":"prompt-engineering/","title":"Prompt Engineering","text":""},{"location":"prompt-engineering/#overview","title":"Overview","text":"<p>Prompt engineering is an emerging discipline focused on developing optimized prompts to efficiently apply language models to various tasks. Prompt engineering helps researchers understand the abilities and limits of large language models (LLMs). By using various prompt engineering techniques, you can often get better answers from the foundation models without spending effort and cost on retraining or fine-tuning them.</p> <p>Note that prompt engineering does not involve fine-tuning the model. In fine-tuning, the weights/parameters are adjusted using training data with the goal of optimizing a cost function. Model fine-tuning is generally an expensive process in terms of computation time and actual costs. Prompt engineering attempts to guide the trained foundation model (FM) to provide more relevant and accurate answers using various methods, such as, better worded questions, similar examples, intermediate steps, and logical reasoning.</p> <p>Prompt Engineering leverages the principle of \u201cpriming\u201d: providing the model with a context of few (3-5) examples of what the user expects the output to look like, so that the model mimics the previously \u201cprimed\u201d behavior. By interacting with the LLM through a series of questions, statements, or instructions, users can effectively guide the LLM's understanding and adjust its behavior according to the specific context of the conversation.</p> <p>In short, prompt engineering is a new and important field for optimizing how you apply, develop, and understand language models, especially large language models. At its core, it is about designing prompts and interactions to expand what language technologies can do, address their weaknesses, and gain insights into their functioning. Prompt engineering equips us with strategies and techniques for pushing the boundaries of what is possible with language models and their applications.</p>"},{"location":"prompt-engineering/#why-is-it-relevant","title":"Why is it relevant?","text":"<p>The key ideas are:</p> <ul> <li>Prompt engineering is the fastest way to harness the power of large language models.</li> <li>Prompt engineering optimizes how you work with and direct language models.</li> <li>It boosts abilities, improves safety, and provides understanding.</li> <li>Prompt engineering incorporates various skills for interfacing with and advancing language models.</li> <li>Prompt engineering enables new features like augmenting domain knowledge with language models without changing model parameters or fine-tuning.</li> <li>Prompt engineering provides methods for interacting with, building with, and grasping language models' capabilities.</li> <li>Higher quality prompt inputs lead to higher quality outputs. In this guide, you will focus on best practices for prompt engineering with several models in Bedrock, including Amazon Titan models and third party models provided by Anthropic and Mistral AI.</li> </ul>"},{"location":"prompt-engineering/#structure-of-a-prompt","title":"Structure of a prompt","text":"<p>As you explore prompt engineering examples, note that each prompt contains:</p> <ul> <li>Instructions: A task for the model to do. (Task description or instruction on how the model should perform)</li> <li>Context: External information to guide the model.</li> <li>Input data: The input you want a response for.</li> <li>Output indicator: The output type or format. Prompts don't require all four elements. Their structure depends on the task. You'll go through some examples soon.</li> </ul> <p></p> <p>With experimentation, you'll gain intuition for crafting and optimizing prompts to best suit your needs and models. Prompt engineering is an iterative skill that improves with practice.</p>"},{"location":"prompt-engineering/#prompt-engineering-patterns","title":"Prompt Engineering Patterns","text":""},{"location":"prompt-engineering/#zero-shot","title":"Zero-Shot","text":"<p>Zero Shot prompting describes the technique where you present a task to an LLM without giving it further examples. You, therefore, expect it to perform the task without getting a prior look at a \u201cshot\u201d at the task, hence the name \u201czero-shot\u201d prompting.</p> <p>Modern LLMs demonstrate remarkable zero-shot performance, and a positive correlation can be drawn between model size and zero-shot performance.</p> <p><pre><code>Human:\nSulfuric acid reacts with sodium chloride, and gives &lt;chemical1&gt;_____&lt;/chemical1&gt; and &lt;chemical2&gt;_____&lt;/chemical2&gt;:\nAssistant: the chemical1 and chemical 2 are:\n</code></pre> Example executed on Amazon Bedrock text playground, with Anthropic Claude model:</p> <p></p>"},{"location":"prompt-engineering/#few-shot","title":"Few-Shot","text":"<p>Giving the model more information about the tasks at hand via examples is called Few-Shot Prompting. It can be used for in-context learning by providing examples of the task and the desired output. You can therefore condition the model on the examples to follow the task guidance more closely.</p> <p><pre><code>Tweet: \"I hate it when my phone battery dies.\u201d: Sentiment: Negative\nTweet: \"My day has been great\u201d: Sentiment: Positive\nTweet: \"This is the link to the article\u201d: Sentiment: Neutral\nTweet: \"This new music video was incredible\u201d Sentiment:\n</code></pre> Example executed in the Amazon Bedrock text playground, using the Amazon Titan model.</p> <p></p>"},{"location":"prompt-engineering/#chain-of-thought-with-few-shot","title":"Chain-of-Thought (with Few-Shot)","text":"<p>Chain-of-Thought (CoT) prompting breaks down complex reasoning tasks through intermediary reasoning steps. Chain-of-Thought prompts are usually very specific to a problem type. One can try to invoke CoT reasoning by using the trigger phrase \"(Think Step-by-Step)\". Let's examine the following example of a few-shot CoT prompt.</p>"},{"location":"prompt-engineering/#hint-you-can-test-following-prompt-in-amazon-bedrock-text-playground-with-distinct-models","title":"Hint: You can test following prompt in Amazon Bedrock text playground with distinct models","text":"<p><pre><code>On a given week, the viewers for a TV channel were\nMonday: 6500 viewers\nTuesday: 6400 viewers\nWednesday: 6300 viewers\n\n\nQuestion: How many viewers can we expect on Friday?\nAnswer: Based on the numbers given and without any more information, there is a daily decrease of 100 viewers. If we assume this trend will continue during the following days, we can expect 6200 viewers on the next day that would be Thursday, and therefore 6100 viewers on the next day that would be Friday.\n\n\nQuestion: How many viewers can we expect on Saturday? (Think Step-by-Step)\nAnswer:\n</code></pre> Answer provided by Anthropic Claude model:</p> <pre><code>Here are the step-by-step workings:\n\n1. Monday viewers: 6500\n2. Tuesday viewers: 6400 (decrease of 100 from Monday)\n3. Wednesday viewers: 6300 (decrease of 100 from Tuesday)\n\nWe see a daily decrease of 100 viewers from the previous day.\n\n4. Following the trend, Thursday viewers: 6200 (decrease of 100 from Wednesday)\n5. Friday viewers: 6100 (decrease of 100 from Thursday)\n6. For Saturday, we continue the decreasing trend and subtract another 100 viewers from Friday:\n\nSaturday viewers: 6000 (decrease of 100 from 6100 on Friday)\nSo based on the decreasing trend of 100 viewers per day, we can estimate 6000 viewers on Saturday.\nThe step-by-step workings are:\n\nMonday: 6500\nTuesday: 6400  (6500 - 100)\nWednesday: 6300 (6400 - 100)\nThursday: 6200 (6300 - 100)\nFriday: 6100 (6200 - 100)\nSaturday: 6000 (6100 - 100)\n\nDoes this make sense? Let me know if you need any clarification.\n</code></pre>"},{"location":"prompt-engineering/#conclusion","title":"Conclusion","text":"<p>You have learned examples of how to engineer prompts to get better results from models. These examples are just a few of the options that prompt engineering can provide. You can test other options in the Amazon Bedrock text playground.</p>"},{"location":"text-generation/","title":"Text Generation","text":""},{"location":"text-generation/#overview","title":"Overview","text":"<p>In this lab, you will learn how to generate text using LLMs on Amazon Bedrock. We will demonstrate the use of LLMs to showcase open-ended text generation and entity extraction use-cases. These examples leverage boto3 api calls. For open source implementations refer to the OpenSource Examples</p> <p>To continue supporting best practices in the responsible use of AI, Titan FMs are built to detect and remove harmful content in the data, reject inappropriate content in the user input, and filter the models\u2019 outputs that contain inappropriate content (such as hate speech, profanity, and violence).</p> <p>AWS offers Amazon Titan and select third-party models in Bedrock. The large language models (LLMs) can be used for tasks such as text generation (e.g. e-mail generation, short stories, essays, social media posts, and web page copy).</p> <p>Entity Extraction Classic entity extraction programs usually limit you to pre-defined classes, such as name, address, price, etc. or require you to provide many examples of types of entities you are interested in. By using an LLM for entity extraction, in most cases you only need to specify what you want to extract in natural language. This gives you flexibility and accuracy in your queries while saving time by removing the necessity of data labeling. In this module, you will be able to work with the Amazon Bedrock API to extract entities based on simple descriptions.</p>"},{"location":"text-generation/#relevance","title":"Relevance","text":"<p>Generating text using large language models results in productivity gains. For example, marketing teams can create an ad copy in minutes using natural language instructions.</p>"},{"location":"text-generation/#target-audience","title":"Target audience","text":"<p>Architects, data scientists, and developer who want to learn how to use Amazon Bedrock LLMs to generate text. Some of the business use cases for text generation include:</p> <ul> <li>Marketing teams - Product descriptions based on data about its features and benefits, social media posts.</li> <li>Media - Automated article generation, Campaigns.</li> <li>All business units - Email generation, report generation.</li> <li>Code Translation, Code explain and reviews</li> <li>Entity extraction</li> </ul>"},{"location":"text-generation/#challenges","title":"Challenges","text":"<p>Generated text can be generic, lacking context, and contain hallucination, which happens when the model generates text output that doesn't correspond with the sent prompt.</p>"},{"location":"text-generation/#architecture","title":"Architecture","text":"<p>The approach will leverage Bedrock native APIs using the Boto3 SDK. We will demonstrate several patterns with this lab which are detailed below. At a high level the flow of prompt and data can be represented by the diagram below</p> <p></p>"},{"location":"text-generation/#sub-patterns","title":"Sub-patterns","text":"<p>There are five sub-patterns that will be demonstrated on this module for text generation:</p> <p>Zero-shot generation  - With zero-shot generation, the user will only provide an input request to generate an email without any context. We will explore zero-shot email generation using two approaches: Bedrock API (Boto3) and Bedrock integration with LangChain.</p> <p>Code generation  - With zero-shot generation we can leverage the code generation capability of the model. This is based on the data learnt by the model during training.</p> <p>Summarization  - In this sub-pattern, we will explore the summarization capability and leverage the Titan model for the same. This approach is dependent on the size of the document and the context length. Later on we will explore other patterns to summarize larger documents.</p> <p>Simplified Question Answering  - The example will show how questions are sent to the model and will get answers from the base model with no modifications. Question Answering (QA) is an important task that involves extracting answers to factual queries posed in natural language. Typically, a QA system processes a query against a knowledge base containing structured or unstructured data and generates a response with accurate information. Ensuring high accuracy is key to developing a useful, reliable and trustworthy question answering system, especially for enterprise use cases.</p> <p>Entity Extraction   - In this sub-pattern, we will Entity extraction is an NLP technique that allows us to automatically extract specific data from naturally written text, such as news, emails, books, etc. That data can then later be saved to a database, used for lookup or any other type of processing.</p>"},{"location":"setup/aws-console/","title":"AWS Console","text":"<p>If you are running this workshop in an AWS event, an AWS account will be provisioned for you.</p> <p>To get started, go to AWS Console. Go to Amazon Bedrock console and on left menu, click on Model access:</p> <p></p> <p>On the Model access screen, click on top right button \"Manage model access\":</p> <p></p> <p>On model access screen, select only following models and click on \"Request model access\" button:</p> <p>Amazon - Titan Embeddings G1 - Text - Titan Text G1 - Express Anthropic - Claude - Claude Instant Stability AI - SDXL 1.0 Mistral AI - Mistral 7B Instruct</p> <p></p> <p>Follow the below steps to launch SageMaker Studio Classic environment - Launch Amazon SageMaker console and click on Domains in the left pane</p> <p></p> <p>Click on the already created Domain amazon-bedrock-workshop If not Please create your domain</p> <p></p> <p>Under User profiles, click on the Launch dropdown and click on Studio to launch studio in another window </p> <p>Within the SageMaker Studio window, click on Studio Classic under Applications to launch SageMaker Studio Classic </p> <p>Click on Open to launch the SageMaker Studio Classic instance </p> <p>Click on File Browser to view already cloned git repository and to launch available notebooks </p> <p>Close the samples from github repo</p> <p>If you are running the workshop in SageMaker Studio (Classic), here are the recommended kernel configurations: Image: Data Science 3.0 (or greater) Instance Type: ml.t3.medium</p> <p></p>"},{"location":"setup/command-line/","title":"AWS CLI Setup","text":"<p>Working in progress</p>"},{"location":"src/00_Prerequisites/","title":"Introduction to Bedrock","text":"<p>This lab will walk you through the basics of connecting to the Amazon Bedrock service from Python with boto3.</p> <p>First, ensure you've completed the setup in the 'Getting Started' section of the root README</p> <p>Then, you'll be ready to walk through the notebook bedrock_basics.ipynb, which shows how to install the required packages, connect to Bedrock, and invoke models. Before running any of the labs ensure you've run the Bedrock boto3 setup notebook.</p>"},{"location":"src/01_Text_generation/","title":"Lab 1 - Text Generation","text":""},{"location":"src/01_Text_generation/#overview","title":"Overview","text":"<p>In this lab, you will learn how to generate text using LLMs on Amazon Bedrock by using the Bedrock API. </p> <p>We will first generate text using a zero-shot prompt. The zero-shot prompt provides instruction to generate text content without providing a detailed context. We will explore zero-shot email generation using Bedrock API (BoTo3). Then we will show how to improve the quality of the generated text by providing additional context in the prompt. Additionally, we will also look at text summarization, name entity recognition, and code generation examples.</p>"},{"location":"src/01_Text_generation/#audience","title":"Audience","text":"<p>Architects, data scientists, and developer who want to learn how to use Amazon Bedrock LLMs to generate text.  Some of the business use cases for text generation include:</p> <ul> <li>Generating product descriptions based on product features and benefits for marketing teams</li> <li>Generation of media articles and marketing campaigns</li> <li>Email and reports generation</li> <li>Code Translation, code explain and reviews</li> </ul>"},{"location":"src/01_Text_generation/#workshop-notebooks","title":"Workshop Notebooks","text":"<p>We will generate an email response to a customer where the customer had provided negative feedback on service received from a customer support engineer. The text generation workshop includes the following three notebooks.  1. Generate Email with Amazon Titan - Invokes Amazon Titan large text model using Bedrock API to generate an email response to a customer. It uses a zero-shot prompt without context as instruction to the model.  2. Zero-shot Text Generation with Anthropic Claude - Invokes Anthropic's Claude Text model using Bedrock API to generate Python code using Natural language. It shows examples of prompting to generate simple functions, classes, and full programs in Python for Data Analyst to perform sales analysis on a given Sales CSV dataset. 3. Text summarization with Amazon Titan and Anthropic Claude - Invoke Amazon Titan large text model and Anthropic's Claude Text model using Bedrock API to generate summary of provided text. 4. Question and answers with Bedrock using Amazon Titan - Invoke Amazon Titan large text model to answer questions using models knowledge, check an example of hallucination, and using prompt engineering to address hallcination. 5. Entity Extraction with Anthropic Claude - Invoke Anthropic's Claude Text model to extract name of book from a given email text.</p>"},{"location":"src/01_Text_generation/#setup","title":"Setup","text":"<p>Before running any of the labs in this section ensure you've run the Bedrock boto3 setup notebook.</p>"},{"location":"src/01_Text_generation/#architecture","title":"Architecture","text":""},{"location":"src/02_KnowledgeBases_and_RAG/","title":"Amazon Bedrock Knowledge Base - Samples for building RAG workflows","text":""},{"location":"src/02_KnowledgeBases_and_RAG/#contents","title":"Contents","text":"<ul> <li> <p>0_create_ingest_documents_test_kb.ipynb - creates necessary role and policies required using the <code>utility.py</code> file. It uses the roles and policies to create Open Search Serverless vector index, knowledge base, data source, and then ingests the documents to the vector store. Once the documents are ingested it will then test the knowledge base using <code>RetrieveAndGenerate</code> API for question answering, and <code>Retrieve</code> API for fetching relevant documents. Finally, it deletes all the resources. If you want to continue with other notebooks, you can choose not to delete the resources and move to other notebooks. Please note, that if you do not delete the resources, you may be incurred cost of storing data in OpenSearch Serverless, even if you are not using it. Therefore, once you are done with trying out the sample code, make sure to delete all the resources. </p> </li> <li> <p>1_managed-rag-kb-retrieve-generate-api.ipynb - Code sample for managed retrieval augmented generation (RAG) using <code>RetrieveAndGenerate</code> API from Knowledge Bases for Amazon Bedrock.</p> </li> <li> <p>2_customized-rag-retrieve-api-claude-v2.ipynb - If you want to customize your RAG workflow, you can use the <code>retrieve</code> API provided by Knowledge Bases for Amazon Bedrock. Use this code sample as a starting point.</p> </li> <li> <p>3_customized-rag-retrieve-api-langchain-claude-v2.ipynb - Code sample for using the <code>RetrieveQA</code> chain from LangChain and Amazon Knowledge Base as the retriever.</p> </li> </ul> <p>Remember to use the 4_CLEAN_UP.ipynb</p>"},{"location":"src/02_KnowledgeBases_and_RAG/#note","title":"Note","text":"<p>If you use the notebook - 0_create_ingest_documents_test_kb.ipynb for creating the knowledge bases and do not delete the resources, you may be incurred cost of storing data in OpenSearch Serverless, even if you are not using it. Therefore, once you are done with trying out the sample code, make sure to delete all the resources. </p>"},{"location":"src/02_KnowledgeBases_and_RAG/#contributing","title":"Contributing","text":"<p>We welcome community contributions! Please ensure your sample aligns with  AWS best practices, and please update the Contents section of this README file with a link to your sample, along with a description.</p>"},{"location":"src/03_Model_customization/","title":"Lab 10 - Custom Models","text":"Warning: This module cannot be executed in Workshop Studio Accounts, and you will have to run this notebook in your own account."},{"location":"src/03_Model_customization/#overview","title":"Overview","text":"<p>Model customization is the process of providing training data to a model in order to improve its performance for specific use-cases. You can customize Amazon Bedrock foundation models in order to improve their performance and create a better customer experience. Amazon Bedrock currently provides the following customization methods.</p> <ul> <li> <p>Fine-tuning</p> <p>Provide labeled data in order to train a model to improve performance on specific tasks. By providing a training dataset of labeled examples, the model learns to associate what types of outputs should be generated for certain types of inputs. The model parameters are adjusted in the process and the model's performance is improved for the tasks represented by the training dataset.</p> </li> <li> <p>Continued Pre-training </p> <p>Provide unlabeled data to pre-train a foundation model by familiarizing it with certain types of inputs. You can provide data from specific topics in order to expose a model to those areas. The Continued Pre-training process will tweak the model parameters to accommodate the input data and improve its domain knowledge. For example, you can train a model with private data, such as business documents, that are not publically available for training large language models. Additionally, you can continue to improve the model by retraining the model with more unlabeled data as it becomes available.</p> </li> </ul>"},{"location":"src/03_Model_customization/#relevance","title":"Relevance","text":"<p>Using your own data, you can privately and securely customize foundation models (FMs) in Amazon Bedrock to build applications that are specific to your domain, organization, and use case. Custom models enable you to create unique user experiences that reflect your company\u2019s style, voice, and services.</p> <ul> <li>With fine-tuning, you can increase model accuracy by providing your own task-specific labeled training dataset and further specialize your FMs. </li> <li>With continued pre-training, you can train models using your own unlabeled data in a secure and managed environment with customer managed keys. Continued pre-training helps models become more domain-specific by accumulating more robust knowledge and adaptability\u2014beyond their original training.</li> </ul> <p>This module walks you through how to customize models through fine-tuning and continued pre-training, how to provision the custom models with provisioned throughput, and how to compare and evaluate model performance. </p>"},{"location":"src/03_Model_customization/#target-audience","title":"Target Audience","text":"<p>This module can be executed by any developer familiar with Python, also by data scientists and other technical people who aspire to customize FMs in Bedrock. </p>"},{"location":"src/03_Model_customization/#setup","title":"Setup","text":"<ul> <li>In this module, please run the 00_setup.ipynb first to make sure resources are properly set up for the following notebooks in this lab.</li> <li>At the end of the module, please run the 04_cleanup.ipynb to make sure resources are removed to avoid unnecessary costs.</li> </ul>"},{"location":"src/03_Model_customization/#patterns","title":"Patterns","text":"<p>On this workshop, you will be able to learn following patterns on customizing FMs in Bedrock:</p> <ol> <li>Fine-tune and Evaluate Llama2 in Bedrock for Summarization: Demonstrates an end-to-end workflow for fine-tuning, provisioning and evaluating a Meta Llama2 in Amazon Bedrock.</li> </ol>"},{"location":"src/04_Image_and_Multimodal/","title":"Lab 3 - Image Generation and Multimodal Embeddings","text":""},{"location":"src/04_Image_and_Multimodal/#overview","title":"Overview","text":"<p>Image generation can be a tedious task for artists, designers and content creators who illustrate their thoughts with the help of images. With the help of Foundation Models (FMs) this tedious task can be streamlined to just a single line of text that expresses the thoughts of the artist, FMs can be used for creating realistic and artistic images of various subjects, environments, and scenes from language prompts.</p> <p>Image indexing and searching is another tedious enterprise task. With the help of FMs, enterprise can build multimodal image indexing, searching and recommendation applications quickly. </p> <p>In this lab, we will explore how to use FMs available in Amazon Bedrock to generate images as well as modify existing images, and how to use FMs to do multimodal image indexing and searching.</p>"},{"location":"src/04_Image_and_Multimodal/#prompt-engineering-for-images","title":"Prompt Engineering for Images","text":"<p>Writing a good prompt can sometimes be an art. It is often difficult to predict whether a certain prompt will yield a satisfactory image with a given model. However, there are certain templates that have been observed to work. Broadly speaking, a prompt can be roughly broken down into three pieces: </p> <ul> <li>type of image (photograph/sketch/painting etc.), and</li> <li>description (subject/object/environment/scene etc.), and</li> <li>the style of the image (realistic/artistic/type of art etc.). </li> </ul> <p>You can change each of the three parts individually, to generate variations of an image. Adjectives have been known to play a significant role in the image generation process. Also, adding more details help in the generation process.To generate a realistic image, you can use phrases such as \"a photo of\", \"a photograph of\", \"realistic\" or \"hyper realistic\". </p> <p>To generate images by artists you can use phrases like \"by Pablo Picasso\" or \"oil painting by Rembrandt\" or \"landscape art by Frederic Edwin Church\" or \"pencil drawing by Albrecht D\u00fcrer\". You can also combine different artists as well. To generate artistic image by category, you can add the art category in the prompt such as \"lion on a beach, abstract\". Some other categories include \"oil painting\", \"pencil drawing\", \"pop art\", \"digital art\", \"anime\", \"cartoon\", \"futurism\", \"watercolor\", \"manga\" etc. You can also include details such as lighting or camera lens, such as 35mm wide lens or 85mm wide lens and details about the framing (portrait/landscape/close up etc.).</p> <p>Note that the model generates different images even if same prompt is given multiple times. So, you can generate multiple images and select the image that suits your application best.</p>"},{"location":"src/04_Image_and_Multimodal/#foundation-models","title":"Foundation Models","text":"<p>To provide these capabilities, Amazon Bedrock supports Stable Diffusion XL from Stability AI and Titan Image Generator from Amazon for image generation, and Titan Multimodal Embeddings for multimodal image indexing and searching.</p>"},{"location":"src/04_Image_and_Multimodal/#stable-diffusion","title":"Stable Diffusion","text":"<p>Stable Diffusion works on the principle of diffusion and is composed of multiple models each having different purpose:</p> <ol> <li>The CLIP text encoder;</li> <li>The VAE decoder;</li> <li>The UNet, and</li> <li>The VAE_post_quant_conv</li> </ol> <p>The workings can be explained with this architecture: </p>"},{"location":"src/04_Image_and_Multimodal/#titan-image-generator","title":"Titan Image Generator","text":"<p>Titan Image Generator G1 is an image generation model. It generates images from text, and allows users to upload and edit an existing image. Users can edit an image with a text prompt (without a mask) or parts of an image with an image mask, or extend the boundaries of an image with outpainting. It can also generate variations of an image.</p>"},{"location":"src/04_Image_and_Multimodal/#titan-multimodal-embeddings","title":"Titan Multimodal Embeddings","text":"<p>Titan Multimodal Embeddings Generation 1 (G1) is a multimodal embeddings model for use cases like searching images by text, image, or a combination of text and image. Designed for high accuracy and fast responses, this model is an ideal choice for search and recommendations use cases.</p>"},{"location":"src/04_Image_and_Multimodal/#target-audience","title":"Target Audience","text":"<p>Marketing companies, agencies, web-designers, and general companies can take advantage of this feature to generate brand new images, from scratch.</p>"},{"location":"src/04_Image_and_Multimodal/#patterns","title":"Patterns","text":"<p>In this workshop, you will be able to learn about Image Generation using Amazon Bedrock starting with text or image input. Use Stable Diffusion as an example in the below graph, and Titan Image Generator can also be used for the same purpose. You will also learn about multimodal image indexing and searching. Note that until the time of preparing this workshop, only Titan Image Generator supports outpainting.</p> <ol> <li>Text to Image     </li> <li>Image to Image (Inpainting and Outpainting)     </li> <li>Multimodal Embeddings     </li> </ol>"},{"location":"src/04_Image_and_Multimodal/#setup","title":"Setup","text":"<p>Before running any of the labs in this section ensure you've run the Bedrock boto3 setup notebook.</p>"},{"location":"src/04_Image_and_Multimodal/#helper","title":"Helper","text":"<p>To facilitate image generation, there is a utility class <code>Bedrock</code> implementation in <code>./utils/bedrock.py</code>. This helps you to generate images easily.</p> <p>You can also explore different <code>style_preset</code>  options here.</p>"},{"location":"src/05_Agents/","title":"Lab 7 - Agents for Bedrock","text":""},{"location":"src/05_Agents/#overview","title":"Overview","text":"<p>In this lab, you will learn about Agents for Amazon Bedrock, a new Amazon Bedrock capability that lets you harness the Foundation Model's (FM's) reasoning skills to execute multi-steps business tasks using natural language. You can simply state your problem, like \u201chelp me update my product catalogue\u201d and the agent breaks down the problem using the FM\u2019s reasoning capabilities and executes the steps to fulfill your request. You set up an agent with access to your organization\u2019s enterprise systems, processes, knowledge bases, and some building block functions. Then the agent comes up with the logic, figures out what APIs to call and when to call them, and completes the transactions in the right sequence. When an agent needs a piece of information from the user, it automatically asks the user for those additional details using natural language. And the best part about agents \u2014 it\u2019s leveraging the most up to date information you have and gives you relevant answers securely and privately. </p> <p>An agent consists of the following components:</p> <ul> <li>Foundation model \u2013 You choose a foundation model that the agent invokes to interpret user input and subsequent prompts in its orchestration process, and to generate responses and follow-up steps in its process.</li> <li>Instructions \u2013 You write up instructions that describe what the agent is designed to do. With advanced prompts, you can further customize instructions for the agent at every step of orchestration and include Lambda functions to parse the output of each step.</li> <li> <p>(Optional) Action groups \u2013 You define the actions that the agent should carry out through providing two resources.</p> <ul> <li>An OpenAPI schema to define the APIs that the agent can invoke to carry out its tasks.</li> <li> <p>A Lambda function with the following input and output.</p> <ul> <li>Input \u2013 The API and parameters identified during orchestration.</li> <li>Output \u2013 The result of the API invocation.</li> </ul> </li> </ul> </li> <li> <p>(Optional) Knowledge bases \u2013 Associate knowledge bases with an agent to allow it to query the knowledge base for extra context to augment response generation and input into steps of the orchestration process.</p> </li> </ul> <p>The following image schematizes the components of your agent.</p> <p></p> <p>In build-time, all these components are gathered to construct base prompts for the agent in order to carry out orchestration until the user request is completed. With advanced prompts, you can modify these base prompts with additional logic and few-shot examples to improve accuracy for each step of agent invocation. The base prompt templates contain instructions, action descriptions, knowledge base descriptions, and conversation history, all of which you can customize to modify the agent to the best of your needs. You then prepare your agent, which packages all the components of the agents, including security configurations, and brings the agent into a state where it is ready for testing in runtime.</p>"},{"location":"src/05_Agents/#audience","title":"Audience","text":"<p>Architects, data scientists and developer who want to learn how to use Agents for Amazon Bedrock to create generative AI applications.  From the simplest instruction only agent to complex assistants that combine Action Groups with Knowledge Bases, you can use the power of agents to quickly develop your Generative API application.</p>"},{"location":"src/05_Agents/#workshop-notebooks","title":"Workshop Notebooks","text":"<p>During this workshop, you will cover two modules:</p> <ol> <li>Building Agents for Bedrock using Boto3 SDK: In this module, you will create agents for Bedrock programmatically using the insurance claim agent example. The files for this module are available in the <code>insurance_claims_agent/without_kb</code> folder</li> <li>Integrating Knowledge Bases to your Agents: In this module, you will create and integrate a Knowledge Base to your insurance claims agent via Boto3 SDK. The files for this module are available in the <code>insurance_claims_agent/with_kb</code> folder.</li> </ol>"},{"location":"src/05_Agents/#setup","title":"Setup","text":"<p>Before running any of the labs in this section ensure you've run the Bedrock boto3 setup notebook.</p>"},{"location":"src/05_Agents/#architectures","title":"Architectures","text":"<p>Building Agents for Bedrock using Boto3 SDK </p> <p>Integrating Knowledge Bases to your Agents </p>"},{"location":"src/05_Agents/insurance_claims_agent/with_kb/","title":"Lab 7.3 - Integrating Knowledge Bases to your Agents","text":""},{"location":"src/05_Agents/insurance_claims_agent/with_kb/#overview","title":"Overview","text":"<p>In this lab we will demonstrate how to integrate a Knowledge Base for Amazon Bedrock to your Agents via AWS Boto3 SDK</p> <p>Knowledge base for Amazon Bedrock provides you the capability of amass  data sources into a repository of information. With knowledge bases, you  can easily build an application that takes advantage of retrieval  augmented generation (RAG), a technique in which the retrieval of  information from data sources augments the generation of model responses.  Once set up, you can take advantage of a knowledge base in the following  ways.</p> <p>Configure your RAG application to use the RetrieveAndGenerate API to query  your knowledge base and generate responses from the information it  retrieves.</p> <p>Associate your knowledge base with an agent (for more information, see  Agents for Amazon Bedrock) to add RAG capability to the agent by helping  it reason through the steps it can take to help end users.</p> <p>Create a custom orchestration flow in your application by using the  Retrieve API to retrieve information directly from the knowledge base.</p> <p>In this lab you will:</p> <ol> <li>Create an Amazon OpenSearch Serverless vector database </li> <li>Create an index for your vector database to perform vector search</li> <li>Create your knowledge base and its required IAM role</li> <li>Create a data source from s3 files and associate it to your knowledge base</li> <li>Ingest the data from S3 to your knowledge base</li> <li>Associate your knowledge base to your agent</li> <li>Invoke your agent with a query that requires knowledge base access</li> </ol> <p>This folder contains the API schema, AWS Lamdbda function and notebook,  <code>create_and_invoke_agent_with_kb</code> with the code for the use case.</p> <p>You can find detailed instructions on the Bedrock Workshop.</p>"},{"location":"src/05_Agents/insurance_claims_agent/without_kb/","title":"Lab 7.2 - Building Agents for Bedrock using Boto3 SDK","text":""},{"location":"src/05_Agents/insurance_claims_agent/without_kb/#overview","title":"Overview","text":"<p>In this lab we will demonstrate how to build, test and deploy Agents via AWS Boto3 SDK</p> <p>Boto3 provides two clients for Agents for Bedrock: - AgentsforBedrock represented by <code>bedrock-agent</code> that provides functionalities related to the Agent's configuration and - AgentsforBedrockRuntime represented by <code>bedrock-agent-runtime</code> that provides functionalities related to the Agent's and Knowledge Base's invocation.</p> <p>The table below details the SDK functionalities</p> Functionality Boto3 SKD Client Scope Create, Update, Delete and Prepare Agent bedrock-agent Agent Configuration Associate, Update and Disassociate Agent Knowledge Base bedrock-agent Agent Configuration Create, Update and Delete Agent Action Group bedrock-agent Agent Configuration Create, Update and Delete Agent Alias bedrock-agent Agent Configuration Invoke Agent bedrock-agent-runtime Agent invocation Query Knowledge Base bedrock-agent-runtime Knowledge Base invocation <p>We will perform the following actions using the Boto3 SDK: 1. Create Agent: create an agent using this API by connecting to  the bedrock client.</p> <ol> <li> <p>Create Agent Action Group: create and assign an action group to the agent  (with corresponding lambda and openAPI schema)</p> </li> <li> <p>Prepare Agent: Prepare an agent for deployment.</p> </li> <li> <p>Create Agent Alias: Creating the agent alias to use in the duration of  invoking the agent and getting the response</p> </li> <li> <p>Invoke Agent: Invoke the agent that you created to get a response from  it while it queries from the knowledge base</p> </li> <li> <p>Delete Agent Action Group: Delete an action group from the agent  configuration</p> </li> <li> <p>Delete Agent Alias: Delete an existing alias of the agent</p> </li> <li> <p>Delete Agent Version: Delete any existing versions of the agent</p> </li> <li> <p>Delete Agent: Delete the entire agent</p> </li> </ol> <p>This folder contains the API schema, AWS Lamdbda function and notebook,  <code>create_and_invoke_agent</code> with the code for the use case.</p> <p>You can find detailed instructions on the Bedrock Workshop.</p>"},{"location":"src/06_OpenSource_examples/02_Langchain_Chatbot_examples/","title":"Lab 4 - Conversational Interfaces (Chatbots)","text":""},{"location":"src/06_OpenSource_examples/02_Langchain_Chatbot_examples/#overview","title":"Overview","text":"<p>Conversational interfaces such as chatbots and virtual assistants can be used to enhance the user experience for your customers.Chatbots uses natural language processing (NLP) and machine learning algorithms to understand and respond to user queries. Chatbots can be used in a variety of applications, such as customer service, sales, and e-commerce, to provide quick and efficient responses to users. They can be accessed through various channels such as websites, social media platforms, and messaging apps.</p>"},{"location":"src/06_OpenSource_examples/02_Langchain_Chatbot_examples/#chatbot-using-amazon-bedrock","title":"Chatbot using Amazon Bedrock","text":""},{"location":"src/06_OpenSource_examples/02_Langchain_Chatbot_examples/#use-cases","title":"Use Cases","text":"<ol> <li>Chatbot (Basic) - Zero Shot chatbot with a FM model</li> <li>Chatbot using prompt - template(Langchain) - Chatbot with some context provided in the prompt template</li> <li>Chatbot with persona - Chatbot with defined roles. i.e. Career Coach and Human interactions</li> <li>Contextual-aware chatbot - Passing in context through an external file by generating embeddings.</li> </ol>"},{"location":"src/06_OpenSource_examples/02_Langchain_Chatbot_examples/#langchain-framework-for-building-chatbot-with-amazon-bedrock","title":"Langchain framework for building Chatbot with Amazon Bedrock","text":"<p>In Conversational interfaces such as chatbots, it is highly important to remember previous interactions, both at a short term but also at a long term level.</p> <p>LangChain provides memory components in two forms. First, LangChain provides helper utilities for managing and manipulating previous chat messages. These are designed to be modular and useful regardless of how they are used. Secondly, LangChain provides easy ways to incorporate these utilities into chains. It allows us to easily define and interact with different types of abstractions, which make it easy to build powerful chatbots.</p>"},{"location":"src/06_OpenSource_examples/02_Langchain_Chatbot_examples/#building-chatbot-with-context-key-elements","title":"Building Chatbot with Context - Key Elements","text":"<p>The first process in a building a contextual-aware chatbot is to generate embeddings for the context. Typically, you will have an ingestion process which will run through your embedding model and generate the embeddings which will be stored in a sort of a vector store. In this example we are using a GPT-J embeddings model for this</p> <p></p> <p>Second process is the user request orchestration , interaction,  invoking and returing the results</p> <p></p>"},{"location":"src/06_OpenSource_examples/02_Langchain_Chatbot_examples/#architecture-context-aware-chatbot","title":"Architecture [Context Aware Chatbot]","text":"<p>In this architecture:</p> <ol> <li>The question asked to the LLM, is run through the embeddings model</li> <li>The context documents are embedded using the Amazon Titan Embeddings Model and stored in the vector database.</li> <li>The embedded text is then input to the FM for contextual search and including the chat history</li> <li>The FM model then gives you the results based on the context.</li> </ol>"},{"location":"src/06_OpenSource_examples/02_Langchain_Chatbot_examples/#setup","title":"Setup","text":"<p>Before running any of the labs in this section ensure you've run the Bedrock boto3 setup notebook.</p>"},{"location":"src/06_OpenSource_examples/02_Langchain_Chatbot_examples/#notebooks","title":"Notebooks","text":"<p>This module provides you with 3 notebooks for the same pattern. You can experience conversation with Anthropic Claude as well as Amazon Titan Text Large to experience each the conversational power of each model.</p> <ol> <li>Chatbot using Claude</li> <li>Chatbot using Titan</li> <li>Chatbot using AI21</li> </ol>"},{"location":"src/06_OpenSource_examples/05_OpenSource_agents/","title":"Using Open Source tooling in Amazon Bedrock Workshop","text":"<p>This hands-on workshop, aimed at developers and solution builders, introduces how to leverage foundation models (FMs) through Amazon Bedrock and supporting Open Source libraries. Amazon Bedrock works extremely well with Open source toling like Langchain, LlamaIndex and a variety of Vector Databases. You can also use hybrid approach of leveraging KnowledgeBase Within this series of labs, you'll explore some of the most common usage patterns we are seeing with our customers for Generative AI. We will show techniques for generating text and images, creating value for organizations by improving productivity. This is achieved by leveraging foundation models to help in composing emails, summarizing text, answering questions, building chatbots, and creating images. While the focus of this workshop is for you to gain hands-on experience implementing these patterns via Bedrock APIs and SDKs and with open-source packages like LangChain and FAISS.</p> <p>Labs include:</p> <ul> <li>01 - Text Generation [Estimated time to complete - 45 mins]<ul> <li>Text generation with Bedrock with Langchain</li> <li>Text summarization with Titan and Claude</li> <li>Long Text generation with LCEL chains</li> <li>Code Translation</li> </ul> </li> <li>02 - Langchain and Knowledge bases for RAG [Estimated time to complete - 45 mins]<ul> <li>Managed RAG retrieve and generate example</li> <li>Langchain RAG retireve and generate example</li> </ul> </li> <li>03 - Langchain Chatbots [Estimated time to complete - 30 mins]<ul> <li>Build Chatbots with Claude, Titan and Llama models</li> </ul> </li> <li>04 - Gaurdrails with Open Source [Estimated time to complete - 30 mins]<ul> <li>Leverage NeMo for Gaurdrails</li> </ul> </li> <li>05 - Open source Agents [Estimated time to complete - 30 mins]<ul> <li>Function Caling </li> <li>Open source orchesteration using LlamaIndex and langchain</li> <li>Open source with Retriever and Agents with Claude V3</li> <li>Open Source agents with Claude V3</li> </ul> </li> </ul> <p>You can also refer to these Step-by-step guided instructions on the workshop website.</p>"},{"location":"src/06_OpenSource_examples/05_OpenSource_agents/#getting-started","title":"Getting started","text":""},{"location":"src/06_OpenSource_examples/05_OpenSource_agents/#choose-a-notebook-environment","title":"Choose a notebook environment","text":"<p>This workshop is presented as a series of Python notebooks, which you can run from the environment of your choice:</p> <ul> <li>For a fully-managed environment with rich AI/ML features, we'd recommend using SageMaker Studio. To get started quickly, you can refer to the instructions for domain quick setup.</li> <li>For a fully-managed but more basic experience, you could instead create a SageMaker Notebook Instance.</li> <li>If you prefer to use your existing (local or other) notebook environment, make sure it has credentials for calling AWS.</li> </ul>"},{"location":"src/06_OpenSource_examples/05_OpenSource_agents/#enable-aws-iam-permissions-for-bedrock","title":"Enable AWS IAM permissions for Bedrock","text":"<p>The AWS identity you assume from your notebook environment (which is the Studio/notebook Execution Role from SageMaker, or could be a role or IAM User for self-managed notebooks), must have sufficient AWS IAM permissions to call the Amazon Bedrock service.</p> <p>To grant Bedrock access to your identity, you can:</p> <ul> <li>Open the AWS IAM Console</li> <li>Find your Role (if using SageMaker or otherwise assuming an IAM Role), or else User</li> <li>Select Add Permissions &gt; Create Inline Policy to attach new inline permissions, open the JSON editor and paste in the below example policy:</li> </ul> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"BedrockFullAccess\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\"bedrock:*\"],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> <p>\u26a0\ufe0f Note: With Amazon SageMaker, your notebook execution role will typically be separate from the user or role that you log in to the AWS Console with. If you'd like to explore the AWS Console for Amazon Bedrock, you'll need to grant permissions to your Console user/role too.</p> <p>For more information on the fine-grained action and resource permissions in Bedrock, check out the Bedrock Developer Guide.</p>"},{"location":"src/06_OpenSource_examples/05_OpenSource_agents/#clone-and-use-the-notebooks","title":"Clone and use the notebooks","text":"<p>\u2139\ufe0f Note: In SageMaker Studio, you can open a \"System Terminal\" to run these commands by clicking File &gt; New &gt; Terminal</p> <p>Once your notebook environment is set up, clone this workshop repository into it.</p> <pre><code>sudo yum install -y unzip\ngit clone https://github.com/aws-samples/amazon-bedrock-workshop.git\ncd amazon-bedrock-workshop\n</code></pre>"},{"location":"tags/","title":"Tags","text":"<p>This page shows a list of pages indexed by their tags:</p>"}]}